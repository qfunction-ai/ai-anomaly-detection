{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885cadcd-fa29-4f41-8fd9-e80ffcc2f60f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Input, BatchNormalization, LeakyReLU, Dense, Reshape, Flatten, Activation \n",
    "from tensorflow.keras.layers import Dropout, multiply, GaussianNoise, MaxPooling2D, concatenate, Embedding\n",
    "import time\n",
    "from datetime import date\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85d4148-52c1-4d0d-a92d-227b3a0e09f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_generator(optimizer, max_length):\n",
    "    \n",
    "    generator = Sequential()\n",
    "\n",
    "    generator.add(Dense(max_length, input_dim=max_length, kernel_initializer=initializers.glorot_normal(seed=42)))\n",
    "    generator.add(Activation('tanh'))\n",
    "    \n",
    "    generator.add(Dense(max_length))\n",
    "    generator.add(Activation('tanh'))\n",
    "    \n",
    "    generator.add(Dense(max_length))\n",
    "    generator.add(Activation('tanh'))\n",
    "    \n",
    "    generator.add(Dense(max_length))\n",
    "    generator.add(Activation('tanh'))\n",
    "       \n",
    "    generator.add(Dense(512))\n",
    "    generator.add(Activation('tanh'))\n",
    "\n",
    "    generator.add(Dense(1024))\n",
    "    generator.add(Activation('tanh'))\n",
    "   \n",
    "    generator.add(Dense(max_length, activation='tanh'))\n",
    "    \n",
    "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return generator\n",
    "\n",
    "def get_discriminator(optimizer, max_length):\n",
    "    \n",
    "    discriminator = Sequential()\n",
    "\n",
    "    discriminator.add(Dense(512, input_dim=max_length, kernel_initializer=initializers.glorot_normal(seed=42)))\n",
    "    discriminator.add(Activation('relu'))\n",
    "    discriminator.add(Dropout(0.2))\n",
    "\n",
    "    discriminator.add(Dense(1024))\n",
    "    discriminator.add(Activation('relu'))\n",
    "    discriminator.add(Dropout(0.2))\n",
    "       \n",
    "    discriminator.add(Dense(1024))\n",
    "    discriminator.add(Activation('relu'))\n",
    "    discriminator.add(Dropout(0.2))\n",
    "    \n",
    "    discriminator.add(Dense(1024))\n",
    "    discriminator.add(Activation('relu'))\n",
    "    discriminator.add(Dropout(0.2))\n",
    "\n",
    "    discriminator.add(Dense(max_length))\n",
    "    discriminator.add(Activation('relu'))\n",
    "    discriminator.add(Dropout(0.2))\n",
    "    \n",
    "    discriminator.add(Dense(1))\n",
    "    discriminator.add(Activation('sigmoid'))\n",
    "   \n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return discriminator\n",
    "\n",
    "def get_gan_network(discriminator, generator, optimizer,input_dim):\n",
    "\n",
    "    discriminator.trainable = False   \n",
    "    gan_input = Input(shape=(input_dim,))  \n",
    "    x = generator(gan_input)        \n",
    "    gan_output = discriminator(x)\n",
    "    \n",
    "    gan = Model(inputs=gan_input, outputs=gan_output)    \n",
    "    gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a123c09f-49b7-441a-b55e-e7c06d805dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = open('parsed-sysmon-logs-recent.txt', 'r')\n",
    "lines = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f047b72c-84ea-49f8-b070-4500fc8bc89e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa35ed6d-f044-4bed-a5e3-378542e9558f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_types = lines[0].strip().split('``')\n",
    "model_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca227d77-9c27-45b2-b761-1bcd1da2b84d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = {}\n",
    "for i, model_type in enumerate(model_types):\n",
    "    values = []\n",
    "    for line in lines:\n",
    "        value = line.strip().split('``')[i]\n",
    "        values.append(value)\n",
    "    \n",
    "    vocab = {}\n",
    "\n",
    "    import random\n",
    "    low = 0\n",
    "    high = 1\n",
    "  \n",
    "    # Python Generate List of Random Numbers Between 0 to 1\n",
    "    floatList = [random.uniform(low, high) for _ in range(257)]\n",
    "\n",
    "    for i in range(0, 257):\n",
    "        vocab[i] = floatList[i]\n",
    "    \n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "    \n",
    "    lengths = np.array([len(value.replace('\\n','').replace('\\r','')) for value in values])\n",
    "    max_length = int(lengths.mean() + lengths.std())\n",
    "\n",
    "    x = [np.frombuffer(bytearray(value, 'utf-8'), np.uint8) for value in values]\n",
    "    x = pad_sequences(x, maxlen=max_length, padding='post', value=0, truncating='post')\n",
    "\n",
    "    x_train = []\n",
    "    for item in x:\n",
    "        x_train.append([vocab[i] for i in item])\n",
    "\n",
    "    scaled_x = np.array(x_train, dtype=np.float32)\n",
    "    \n",
    "    learning_rate = 0.00001\n",
    "    batch_size = len(scaled_x)\n",
    "    epochs = 200\n",
    "    adam = tf.keras.optimizers.legacy.Adam(learning_rate = learning_rate,beta_1 = 0.5)\n",
    "    \n",
    "    #Calculating the number of batches based on the batch size\n",
    "    batch_count = len(scaled_x) // batch_size\n",
    "    pbar = tqdm(total=epochs * batch_count)\n",
    "    gan_loss = []\n",
    "    discriminator_loss = []\n",
    "\n",
    "    #Inititalizing the network\n",
    "    generator = get_generator(adam, max_length)\n",
    "    discriminator = get_discriminator(adam, max_length)\n",
    "    gan = get_gan_network(discriminator, generator, adam,input_dim=max_length)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        d_loss= 0       \n",
    "        for index in range(batch_count):        \n",
    "            pbar.update(1)        \n",
    "            # Creating a random set of input noise and images\n",
    "            noise = np.random.normal(0, 1, size=[batch_size,max_length])\n",
    "        \n",
    "            # Generate fake logs\n",
    "            generated_images = generator.predict_on_batch(noise)\n",
    "        \n",
    "            #Obtain a batch of normal logs\n",
    "            image_batch = scaled_x[index * batch_size: (index + 1) * batch_size]\n",
    "            \n",
    "            X = np.vstack((generated_images,image_batch))       \n",
    "            y_dis = np.ones(2*batch_size) \n",
    "            y_dis[:batch_size] = 0\n",
    "\n",
    "            # Train discriminator\n",
    "            discriminator.trainable = True\n",
    "            d_loss= discriminator.train_on_batch(X, y_dis)\n",
    "\n",
    "            # Train generator\n",
    "            noise = np.random.uniform(0, 1, size=[batch_size, max_length])\n",
    "            y_gen = np.ones(batch_size)\n",
    "            discriminator.trainable = False\n",
    "            g_loss = gan.train_on_batch(noise, y_gen)\n",
    "        \n",
    "            #Record the losses\n",
    "            gan_loss.append(g_loss)\n",
    "        discriminator_loss.append(d_loss)\n",
    "        \n",
    "    orig_recon_scores = discriminator.predict(scaled_x, verbose=False)\n",
    "    orig_results = pd.DataFrame(orig_recon_scores, columns=['anomaly_score'])\n",
    "    m = orig_results['anomaly_score'].median()\n",
    "    std = orig_results['anomaly_score'].std()\n",
    "        \n",
    "    models[model_type] = (discriminator, vocab, max_length, m, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0085c9-c008-4fa6-a9c6-96f0d3bf171c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
