{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c91213-3488-4067-9a09-104473fbc752",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Input, BatchNormalization, LeakyReLU, Dense, Reshape, Flatten, Activation \n",
    "from tensorflow.keras.layers import Dropout, multiply, GaussianNoise, MaxPooling2D, concatenate, Embedding\n",
    "import time\n",
    "from datetime import date\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8ef382-8df9-4afb-b07e-bfe7ea15db4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_generator(optimizer, max_length):\n",
    "    \n",
    "    generator = Sequential()\n",
    "\n",
    "    generator.add(Dense(max_length, input_dim=max_length, kernel_initializer=initializers.glorot_normal(seed=42)))\n",
    "    generator.add(Activation('tanh'))\n",
    "    \n",
    "    generator.add(Dense(max_length))\n",
    "    generator.add(Activation('tanh'))\n",
    "    \n",
    "    generator.add(Dense(max_length))\n",
    "    generator.add(Activation('tanh'))\n",
    "    \n",
    "    generator.add(Dense(max_length))\n",
    "    generator.add(Activation('tanh'))\n",
    "       \n",
    "    generator.add(Dense(512))\n",
    "    generator.add(Activation('tanh'))\n",
    "\n",
    "    generator.add(Dense(1024))\n",
    "    generator.add(Activation('tanh'))\n",
    "   \n",
    "    generator.add(Dense(max_length, activation='tanh'))\n",
    "    \n",
    "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return generator\n",
    "\n",
    "def get_discriminator(optimizer, max_length):\n",
    "    \n",
    "    discriminator = Sequential()\n",
    "\n",
    "    discriminator.add(Dense(512, input_dim=max_length, kernel_initializer=initializers.glorot_normal(seed=42)))\n",
    "    discriminator.add(Activation('relu'))\n",
    "    discriminator.add(Dropout(0.2))\n",
    "\n",
    "    discriminator.add(Dense(1024))\n",
    "    discriminator.add(Activation('relu'))\n",
    "    discriminator.add(Dropout(0.2))\n",
    "       \n",
    "    discriminator.add(Dense(1024))\n",
    "    discriminator.add(Activation('relu'))\n",
    "    discriminator.add(Dropout(0.2))\n",
    "    \n",
    "    discriminator.add(Dense(1024))\n",
    "    discriminator.add(Activation('relu'))\n",
    "    discriminator.add(Dropout(0.2))\n",
    "\n",
    "    discriminator.add(Dense(max_length))\n",
    "    discriminator.add(Activation('relu'))\n",
    "    discriminator.add(Dropout(0.2))\n",
    "    \n",
    "    discriminator.add(Dense(1))\n",
    "    discriminator.add(Activation('sigmoid'))\n",
    "   \n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return discriminator\n",
    "\n",
    "def get_gan_network(discriminator, generator, optimizer,input_dim):\n",
    "\n",
    "    discriminator.trainable = False   \n",
    "    gan_input = Input(shape=(input_dim,))  \n",
    "    x = generator(gan_input)        \n",
    "    gan_output = discriminator(x)\n",
    "    \n",
    "    gan = Model(inputs=gan_input, outputs=gan_output)    \n",
    "    gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ef4a47-a0bd-418f-8664-05641890689c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = open('conn.log', 'r')\n",
    "lines = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93067c5c-aaca-44c0-960f-b7917fe89626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_type = {\"dest_host\":4,\"dest_port\":5,\"service_duration\":8,\"orig_bytes\":9,\"resp_bytes\":10,\"orig_pkts\":16,\"resp_pkts\":18}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaedc8cb-ecf6-4b0a-829c-9c54b6fb0bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "dest_hosts = []\n",
    "dest_ports = []\n",
    "service_durations = []\n",
    "orig_bytes = []\n",
    "resp_bytes = []\n",
    "orig_pkts = []\n",
    "resp_pkts = []\n",
    "\n",
    "for line in lines[8:]:\n",
    "    values = line.strip().split('\\t')\n",
    "    dest_hosts.append(values[model_type[\"dest_host\"]].replace('\"',''))\n",
    "    dest_ports.append(values[model_type[\"dest_port\"]].replace('\"',''))\n",
    "    \n",
    "    try:\n",
    "        service_durations.append(float(values[model_type[\"service_duration\"]].replace('\"','')))\n",
    "    except:\n",
    "        service_durations.append(0)\n",
    "    \n",
    "    try:\n",
    "        orig_bytes.append(float(values[model_type[\"orig_bytes\"]].replace('\"','')))\n",
    "    except:\n",
    "        orig_bytes.append(0)\n",
    "        \n",
    "    try:\n",
    "        resp_bytes.append(float(values[model_type[\"resp_bytes\"]].replace('\"','')))\n",
    "    except:\n",
    "        resp_bytes.append(0)\n",
    "        \n",
    "    try:\n",
    "        orig_pkts.append(float(values[model_type[\"orig_pkts\"]].replace('\"','')))\n",
    "    except:\n",
    "        orig_pkts.append(0)\n",
    "        \n",
    "    try:\n",
    "        resp_pkts.append(float(values[model_type[\"resp_pkts\"]].replace('\"','')))\n",
    "    except:\n",
    "        resp_pkts.append(0)\n",
    "    \n",
    "vocab = {}\n",
    "\n",
    "import random\n",
    "low = 0\n",
    "high = 1\n",
    "  \n",
    "# Python Generate List of Random Numbers Between 0 to 1\n",
    "floatList = [random.uniform(low, high) for _ in range(257)]\n",
    "\n",
    "for i in range(0, 257):\n",
    "    vocab[i] = floatList[i]\n",
    "    \n",
    "vocab_size = len(vocab)\n",
    "\n",
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "    \n",
    "dest_hosts_lengths = np.array([len(dest_host.replace('\\n','').replace('\\r','')) for dest_host in dest_hosts])\n",
    "dest_ports_lengths = np.array([len(dest_port.replace('\\n','').replace('\\r','')) for dest_port in dest_ports])\n",
    "dest_hosts_max_length = int(dest_hosts_lengths.mean() + dest_hosts_lengths.std())\n",
    "dest_ports_max_length = 5\n",
    "\n",
    "dest_hosts_x = [np.frombuffer(bytearray(dest_host, 'utf-8'), np.uint8) for dest_host in dest_hosts]\n",
    "dest_hosts_x = pad_sequences(dest_hosts_x, maxlen=dest_hosts_max_length, padding='post', value=0, truncating='post')\n",
    "\n",
    "dest_ports_x = [np.frombuffer(bytearray(dest_port, 'utf-8'), np.uint8) for dest_port in dest_ports]\n",
    "dest_ports_x = pad_sequences(dest_ports_x, maxlen=dest_ports_max_length, padding='post', value=0, truncating='post')\n",
    "\n",
    "dest_hosts_x_train = []\n",
    "for item in dest_hosts_x:\n",
    "    dest_hosts_x_train.append([vocab[i] for i in item])\n",
    "\n",
    "dest_ports_x_train = []\n",
    "for item in dest_ports_x:\n",
    "    dest_ports_x_train.append([vocab[i] for i in item])\n",
    "\n",
    "scaled_dest_hosts = np.array(dest_hosts_x_train, dtype=np.float32)\n",
    "\n",
    "final_arr = []\n",
    "for i, line in enumerate(lines[8:]):\n",
    "    temp_arr = [service_durations[i], orig_bytes[i], resp_bytes[i], orig_pkts[i], resp_pkts[i]]\n",
    "    final_arr.append(temp_arr)\n",
    "final_arr = np.array(final_arr, dtype=np.float32)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_final_arr = scaler.fit_transform(final_arr)\n",
    "\n",
    "scaled_final_arr = np.concatenate((dest_ports_x_train,scaled_final_arr), axis=1)\n",
    "dest_ports_max_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a0af0-56b8-474c-a978-2dbc8908e2df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.00001\n",
    "batch_size = len(scaled_dest_hosts)\n",
    "epochs = 200\n",
    "adam = tf.keras.optimizers.legacy.Adam(learning_rate = learning_rate,beta_1 = 0.5)\n",
    "    \n",
    "#Calculating the number of batches based on the batch size\n",
    "batch_count = len(scaled_dest_hosts) // batch_size\n",
    "pbar = tqdm(total=epochs * batch_count)\n",
    "gan_loss = []\n",
    "discriminator_loss = []\n",
    "\n",
    "#Inititalizing the network\n",
    "generator = get_generator(adam, dest_hosts_max_length)\n",
    "discriminator = get_discriminator(adam, dest_hosts_max_length)\n",
    "gan = get_gan_network(discriminator, generator, adam,input_dim=dest_hosts_max_length)\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    d_loss= 0       \n",
    "    for index in range(batch_count):        \n",
    "        pbar.update(1)        \n",
    "        # Creating a random set of input noise and images\n",
    "        noise = np.random.normal(0, 1, size=[batch_size,dest_hosts_max_length])\n",
    "        \n",
    "        # Generate fake logs\n",
    "        generated_images = generator.predict_on_batch(noise)\n",
    "        \n",
    "        #Obtain a batch of normal logs\n",
    "        image_batch = scaled_dest_hosts[index * batch_size: (index + 1) * batch_size]\n",
    "            \n",
    "        X = np.vstack((generated_images,image_batch))       \n",
    "        y_dis = np.ones(2*batch_size) \n",
    "        y_dis[:batch_size] = 0\n",
    "\n",
    "        # Train discriminator\n",
    "        discriminator.trainable = True\n",
    "        d_loss= discriminator.train_on_batch(X, y_dis)\n",
    "\n",
    "        # Train generator\n",
    "        noise = np.random.uniform(0, 1, size=[batch_size, dest_hosts_max_length])\n",
    "        y_gen = np.ones(batch_size)\n",
    "        discriminator.trainable = False\n",
    "        g_loss = gan.train_on_batch(noise, y_gen)\n",
    "            \n",
    "        #Record the losses\n",
    "        gan_loss.append(g_loss)\n",
    "    discriminator_loss.append(d_loss)\n",
    "        \n",
    "orig_recon_scores = discriminator.predict(scaled_dest_hosts, verbose=False)\n",
    "orig_results = pd.DataFrame(orig_recon_scores, columns=['anomaly_score'])\n",
    "m = orig_results['anomaly_score'].median()\n",
    "std = orig_results['anomaly_score'].std()\n",
    "        \n",
    "#models[model_type[j]] = (discriminator, vocab, max_length, m, std)\n",
    "print(\"Discriminator loss: \")\n",
    "print(discriminator_loss)\n",
    "print('\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753dcb12-543c-452e-86bf-6c3c7e00a275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.00001\n",
    "batch_size = len(scaled_final_arr)\n",
    "epochs = 200\n",
    "adam = tf.keras.optimizers.legacy.Adam(learning_rate = learning_rate,beta_1 = 0.5)\n",
    "    \n",
    "#Calculating the number of batches based on the batch size\n",
    "batch_count = len(scaled_final_arr) // batch_size\n",
    "pbar = tqdm(total=epochs * batch_count)\n",
    "gan_loss = []\n",
    "discriminator_loss = []\n",
    "\n",
    "#Inititalizing the network\n",
    "generator = get_generator(adam, dest_ports_max_length)\n",
    "discriminator = get_discriminator(adam, dest_ports_max_length)\n",
    "gan = get_gan_network(discriminator, generator, adam,input_dim=dest_ports_max_length)\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    d_loss= 0       \n",
    "    for index in range(batch_count):        \n",
    "        pbar.update(1)        \n",
    "        # Creating a random set of input noise and images\n",
    "        noise = np.random.normal(0, 1, size=[batch_size,dest_ports_max_length])\n",
    "        \n",
    "        # Generate fake logs\n",
    "        generated_images = generator.predict_on_batch(noise)\n",
    "        \n",
    "        #Obtain a batch of normal logs\n",
    "        image_batch = scaled_final_arr[index * batch_size: (index + 1) * batch_size]\n",
    "            \n",
    "        X = np.vstack((generated_images,image_batch))       \n",
    "        y_dis = np.ones(2*batch_size) \n",
    "        y_dis[:batch_size] = 0\n",
    "\n",
    "        # Train discriminator\n",
    "        discriminator.trainable = True\n",
    "        d_loss= discriminator.train_on_batch(X, y_dis)\n",
    "\n",
    "        # Train generator\n",
    "        noise = np.random.uniform(0, 1, size=[batch_size, dest_ports_max_length])\n",
    "        y_gen = np.ones(batch_size)\n",
    "        discriminator.trainable = False\n",
    "        g_loss = gan.train_on_batch(noise, y_gen)\n",
    "            \n",
    "        #Record the losses\n",
    "        gan_loss.append(g_loss)\n",
    "    discriminator_loss.append(d_loss)\n",
    "        \n",
    "orig_recon_scores = discriminator.predict(scaled_final_arr, verbose=False)\n",
    "orig_results = pd.DataFrame(orig_recon_scores, columns=['anomaly_score'])\n",
    "connection_median = orig_results['anomaly_score'].median()\n",
    "connection_std = orig_results['anomaly_score'].std()\n",
    "        \n",
    "#models[model_type[j]] = (discriminator, vocab, max_length, m, std)\n",
    "print(\"Discriminator loss: \")\n",
    "print(discriminator_loss)\n",
    "print('\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf543998-f1e2-4383-8890-e045c31108bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f14792-3f6c-4c77-b38b-66b39be69d11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582395ae-eed0-4b72-86f9-af74f7a24ac7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lines[18].split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecf5288-fce3-4f18-8376-e77d082780f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552ecf65-91d1-46d8-96d3-a9c15873c4bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dest_hosts = []\n",
    "dest_ports = []\n",
    "service_durations = []\n",
    "orig_bytes = []\n",
    "resp_bytes = []\n",
    "orig_pkts = []\n",
    "resp_pkts = []\n",
    "\n",
    "for line in lines[8:]:\n",
    "    values = line.strip().split('\\t')\n",
    "    dest_hosts.append(values[model_type[\"dest_host\"]].replace('\"',''))\n",
    "    dest_ports.append(values[model_type[\"dest_port\"]].replace('\"',''))\n",
    "    \n",
    "    try:\n",
    "        service_durations.append(float(values[model_type[\"service_duration\"]].replace('\"','')))\n",
    "    except:\n",
    "        service_durations.append(0)\n",
    "    \n",
    "    try:\n",
    "        orig_bytes.append(float(values[model_type[\"orig_bytes\"]].replace('\"','')))\n",
    "    except:\n",
    "        orig_bytes.append(0)\n",
    "        \n",
    "    try:\n",
    "        resp_bytes.append(float(values[model_type[\"resp_bytes\"]].replace('\"','')))\n",
    "    except:\n",
    "        resp_bytes.append(0)\n",
    "        \n",
    "    try:\n",
    "        orig_pkts.append(float(values[model_type[\"orig_pkts\"]].replace('\"','')))\n",
    "    except:\n",
    "        orig_pkts.append(0)\n",
    "        \n",
    "    try:\n",
    "        resp_pkts.append(float(values[model_type[\"resp_pkts\"]].replace('\"','')))\n",
    "    except:\n",
    "        resp_pkts.append(0)\n",
    "\n",
    "dest_hosts_x = [np.frombuffer(bytearray(dest_host, 'utf-8'), np.uint8) for dest_host in dest_hosts]\n",
    "dest_hosts_x = pad_sequences(dest_hosts_x, maxlen=dest_hosts_max_length, padding='post', value=0, truncating='post')\n",
    "\n",
    "dest_ports_max_length = 5\n",
    "dest_ports_x = [np.frombuffer(bytearray(dest_port, 'utf-8'), np.uint8) for dest_port in dest_ports]\n",
    "dest_ports_x = pad_sequences(dest_ports_x, maxlen=dest_ports_max_length, padding='post', value=0, truncating='post')\n",
    "\n",
    "dest_hosts_x_train = []\n",
    "for item in dest_hosts_x:\n",
    "    dest_hosts_x_train.append([vocab[i] for i in item])\n",
    "\n",
    "dest_ports_x_train = []\n",
    "for item in dest_ports_x:\n",
    "    dest_ports_x_train.append([vocab[i] for i in item])\n",
    "\n",
    "scaled_dest_hosts = np.array(dest_hosts_x_train, dtype=np.float32)\n",
    "\n",
    "final_arr = []\n",
    "for i, line in enumerate(lines[8:]):\n",
    "    temp_arr = [service_durations[i], orig_bytes[i], resp_bytes[i], orig_pkts[i], resp_pkts[i]]\n",
    "    final_arr.append(temp_arr)\n",
    "final_arr = np.array(final_arr, dtype=np.float32)\n",
    "\n",
    "scaled_final_arr = scaler.transform(final_arr)\n",
    "\n",
    "scaled_final_test = np.concatenate((dest_ports_x_train,scaled_final_arr), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09af0ef5-f4f0-4634-a8d9-48bdee72381e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = discriminator.predict(scaled_final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e9b5f4-3afb-4b83-8e9a-52b6dc549c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a536711-ab17-4408-a97b-16d355716575",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "connection_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da416c99-4591-4d42-9349-a13d81cec4e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "connection_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e65bc9a-6839-432e-be91-a7443ddd23fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zscores = []\n",
    "\n",
    "for pred in results:\n",
    "    value = pred[0]\n",
    "    found = False\n",
    "    for i in range(1,10):\n",
    "        threshold = connection_median - (i * connection_std)\n",
    "        if value > threshold:\n",
    "            zscores.append(i)\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        zscores.append(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf530f6-0600-45cb-b55a-4cad5f2785c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in zscores:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aded1286-3eda-4f64-8295-7030aba2f497",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1711394766.979827\tC0sYpa4mZA85wY5bP6\t192.168.100.129\t34100\t143.198.3.13\t11601\ttcp\tssl\t86511.291214\t24089117\t17914541\tS1\t-\t-\t0\tShADadwtt\t326237\t37138617\t345043\t31722626\t-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(lines[3404])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a540cbb-2958-489e-9031-864abf957f32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
